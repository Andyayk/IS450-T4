{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Before closing, go to Cell > All Output > Clear to keep file size small.\n",
    "\n",
    "Also make sure this jupyter notebook file is opened using the following command:\n",
    "\n",
    "```jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating Reduced Datasets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "datasize = 20000\n",
    "\n",
    "df = pd.DataFrame(columns=['itemid', 'title', 'Category', 'image_path']) #creating dataframe\n",
    "\n",
    "all_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# sort the dataframe\n",
    "all_df.sort_values(by='Category', inplace=True)\n",
    "\n",
    "# get a list of category\n",
    "mobilelist=list(range(31, 58))\n",
    "fashionlist=list(range(17,31))\n",
    "beautylist=list(range(0, 17))\n",
    "\n",
    "def retrievesample(all_df, list, df):\n",
    "    eachdf = all_df.loc[all_df.Category.isin(list)]\n",
    "\n",
    "    count_row = eachdf.shape[0]  # gives number of row count\n",
    "    print(\"Original Count:\", count_row)\n",
    "    \n",
    "    eachdf = eachdf.sample(datasize) #retrieve a sample\n",
    "\n",
    "    count_row = eachdf.shape[0]  # gives number of row count\n",
    "    print(\"New Sample Count:\", count_row)\n",
    "    \n",
    "    df = df.append(eachdf, ignore_index=True) #append to original dataframe\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = retrievesample(all_df, mobilelist, df)\n",
    "df = retrievesample(all_df, fashionlist, df)\n",
    "df = retrievesample(all_df, beautylist, df)\n",
    "\n",
    "df.to_csv('train2.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries and reading explored data into Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, io, gensim, datetime, time, nltk, random, pickle\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Libraries for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For Bagging implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#For AdaBoost implementation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#For Random Forest implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "string = 'true:'\n",
    "goldtruth = [string+str(i) for i in range(0, 58)]\n",
    "\n",
    "string = 'pred:'\n",
    "prediction = [string+str(i) for i in range(0, 58)]\n",
    "\n",
    "\n",
    "def printModelAccuracy(y_test, y_pred):\n",
    "    # Find the confusion matrix of the result\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=list(range(0, 58))), \\\n",
    "        index=goldtruth, \n",
    "        columns=prediction)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Find the accuracy and F1 score of the result\n",
    "    asr = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(\"Accuracy:\", asr)\n",
    "    print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Selection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"train2.csv\",header = 0)\n",
    "corpuslist = all_df[\"title\"]\n",
    "\n",
    "titles = []\n",
    "\n",
    "for title in corpuslist:\n",
    "    eachwordintitle = nltk.word_tokenize(title)\n",
    "    titles += eachwordintitle\n",
    "\n",
    "corpuslist = titles\n",
    "\n",
    "#The below code is for the feature set definition. We are using only top 5000 words as our features \n",
    "fdist = nltk.FreqDist(w.lower() for w in corpuslist)\n",
    "\n",
    "totaluniquewords = 0\n",
    "for word in fdist:\n",
    "    totaluniquewords+=1\n",
    "print(\"Total Unique Words:\", totaluniquewords)\n",
    "\n",
    "datasize = 5000\n",
    "\n",
    "mostcommonwords = fdist.most_common()[:datasize] #top 5k\n",
    "mostcommonwords = [w[0] for w in mostcommonwords]\n",
    "\n",
    "middlewords = fdist.most_common()[(datasize):(totaluniquewords-datasize)] #middle 5k\n",
    "middlewords = [w[0] for w in middlewords]\n",
    "middlewords = random.sample(middlewords, datasize)\n",
    "\n",
    "leastcommonwords = fdist.most_common()[-datasize:] #bottom 5k\n",
    "leastcommonwords = [w[0] for w in leastcommonwords]\n",
    "\n",
    "print(\"Total Most Common Words:\", len(mostcommonwords))\n",
    "print(mostcommonwords[:10])\n",
    "\n",
    "print(\"BREAK\")\n",
    "\n",
    "print(\"Total Middle Words:\", len(middlewords))\n",
    "print(middlewords[:10])\n",
    "\n",
    "print(\"BREAK\")\n",
    "\n",
    "print(\"Total Least Common Words:\", len(leastcommonwords))\n",
    "print(leastcommonwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes around 2 minute\n",
    "all_df = pd.read_csv(\"train2.csv\",header = 0)\n",
    "corpus = all_df[\"title\"]\n",
    "labels = all_df[\"Category\"]\n",
    "\n",
    "start = time.time()\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def corpus2docs(corpus):\n",
    "    # corpus is a object returned by load_corpus that represents a corpus.\n",
    "    docs1 = []\n",
    "\n",
    "    for title in corpus:\n",
    "        doc = nltk.word_tokenize(title)\n",
    "        docs1.append(doc)\n",
    "    docs2 = [[w.lower() for w in doc] for doc in docs1] #lower case the words\n",
    "    #docs2b = docs2 #no feature selection\n",
    "    docs2b = [[w for w in doc if w in mostcommonwords] for doc in docs2] #selecting top 5k words as our features\n",
    "    #docs2b = [[w for w in doc if w in middlewords] for doc in docs2] #selecting middle 5k words as our features\n",
    "    #docs2b = [[w for w in doc if w in leastcommonwords] for doc in docs2] #selecting bottom 5k words as our features\n",
    "    docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2b] #removing special characters and numbers\n",
    "    docs4 = [[w for w in doc if w not in stop_list] for doc in docs3] #removing words in stop list\n",
    "    #changing list into a string\n",
    "    docs5 = [' '.join([stemmer.stem(w) for w in doc]) for doc in docs4] #changing the words into its root form\n",
    "    \n",
    "    return docs5\n",
    "\n",
    "docs = corpus2docs(corpus)\n",
    "end = time. time()\n",
    "print(end - start)\n",
    "print(docs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vectorise and TFIDF words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes around 10 seconds\n",
    "start = time.time()\n",
    "\n",
    "def convertToDataframe(listofwords, labels):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word') #tfidf\n",
    "    words_tfidf = vectorizer.fit_transform(listofwords) #tfidf\n",
    "\n",
    "    tablecolumns = []                      \n",
    "    tablecolumns.append(vectorizer.get_feature_names()) #adding column headers\n",
    "\n",
    "    df = pd.DataFrame(words_tfidf.toarray(), columns=tablecolumns) #creating dataframe\n",
    "\n",
    "    df['Category'] = labels\n",
    "                      \n",
    "    return df\n",
    "\n",
    "df = convertToDataframe(docs, labels.values.tolist())\n",
    "print(df.head(10))\n",
    "\n",
    "end = time. time()\n",
    "print(end - start)\n",
    "\n",
    "docs = \"\" #clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train-Test Split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'Category'] #take everything except Category\n",
    "\n",
    "y = df[['Category']] #our label is Category\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "df = \"\" #clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes to compare which features to select</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes = MultinomialNB()\n",
    "#Fit the training feature Xs and training label Ys\n",
    "naivebayes.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Dataframe to file</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"X_train\")\n",
    "X_test.to_pickle(\"X_test\")\n",
    "y_train.to_pickle(\"y_train\")\n",
    "y_test.to_pickle(\"y_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
