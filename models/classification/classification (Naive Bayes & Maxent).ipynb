{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.metrics.scores import *\n",
    "from sklearn.metrics import (f1_score,accuracy_score,precision_score,recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Split data into 60%(training) 20%(testing) 20%(validating)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header = 0 mean row 0(1st row in excel:itemid title Category image_page) is header\n",
    "all_df = pd.read_csv(\"train.csv\",header = 0)\n",
    "\n",
    "#shuffle\n",
    "all_df = all_df.sample(frac = 1)\n",
    "\n",
    "#separate in to 60% of train, 20% of test, 20% of validate\n",
    "total_num = len(all_df)\n",
    "\n",
    "num_of_60_percent_title = int(0.6 * total_num)\n",
    "num_of_20_percent_title = int(0.20 * total_num)\n",
    "#print(num_of_60_percent_title)\n",
    "#print(num_of_20_percent_title)\n",
    "#print(num_of_60_percent_title+num_of_20_percent_title+num_of_20_percent_title)\n",
    "\n",
    "train_df = all_df[0:num_of_60_percent_title]\n",
    "test_df = all_df[num_of_60_percent_title:num_of_60_percent_title+num_of_20_percent_title]\n",
    "val_df = all_df[num_of_60_percent_title+num_of_20_percent_title:]\n",
    "\n",
    "#print(len(train_df))\n",
    "#print(len(test_df))\n",
    "#print(len(val_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data preprocessing for training testing and validating data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#for training data\n",
    "labels = []\n",
    "corpus = []\n",
    "for title in train_df['title']:\n",
    "    sent = nltk.word_tokenize(title)\n",
    "     \n",
    "    sent = [w.lower() for w in sent]\n",
    "    \n",
    "    sent = [w for w in sent if re.search('^[a-z]+$',w)]\n",
    "\n",
    "    sent = [w for w in sent if w not in stop_list]\n",
    "\n",
    "    sent = [stemmer.stem(w) for w in sent]\n",
    "\n",
    "    corpus.append(sent)\n",
    "\n",
    "for category in train_df['Category']:\n",
    "    labels.append(category)\n",
    "    \n",
    "# Create a dictionary from the corpus.\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "# Store the labeled training data in the following list.\n",
    "labeled_training_data = []\n",
    "    \n",
    "# Going through the two lists in parallel to create the labeled data set.\n",
    "for (l, s) in zip(labels, corpus):\n",
    "\n",
    "    # Convert the original sentence into a vector.\n",
    "    vector = dictionary.doc2bow(s)\n",
    "    \n",
    "    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    \n",
    "    # Add the labeled sentence to the labeled data set.\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "#for testing data\n",
    "labels = []\n",
    "corpus = []\n",
    "for title in test_df['title']:\n",
    "    sent = nltk.word_tokenize(title)\n",
    "     \n",
    "    sent = [w.lower() for w in sent]\n",
    "    \n",
    "    sent = [w for w in sent if re.search('^[a-z]+$',w)]\n",
    "\n",
    "    sent = [w for w in sent if w not in stop_list]\n",
    "\n",
    "    sent = [stemmer.stem(w) for w in sent]\n",
    "\n",
    "    corpus.append(sent)\n",
    "\n",
    "for category in test_df['Category']:\n",
    "    labels.append(category)\n",
    "    \n",
    "# Store the labeled test data in the following list.\n",
    "labeled_test_data = []\n",
    "    \n",
    "# Going through the two lists in parallel to create the labeled data set.\n",
    "for (l, s) in zip(labels, corpus):\n",
    "\n",
    "    # Convert the original sentence into a vector.\n",
    "    vector = dictionary.doc2bow(s)\n",
    "    \n",
    "    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    \n",
    "    # Add the labeled sentence to the labeled data set.\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "\n",
    "#print(corpus)    \n",
    "test_tf_vectors = [dictionary.doc2bow(title) for title in corpus]\n",
    "\n",
    "# Convert documents into dict representation. This is document-label representation\n",
    "test_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in test_tf_vectors]\n",
    "test_labels = labels\n",
    "\n",
    "#for validating data\n",
    "labels = []\n",
    "corpus = []\n",
    "for title in val_df['title']:\n",
    "    sent = nltk.word_tokenize(title)\n",
    "     \n",
    "    sent = [w.lower() for w in sent]\n",
    "    \n",
    "    sent = [w for w in sent if re.search('^[a-z]+$',w)]\n",
    "\n",
    "    sent = [w for w in sent if w not in stop_list]\n",
    "\n",
    "    sent = [stemmer.stem(w) for w in sent]\n",
    "\n",
    "    corpus.append(sent)\n",
    "\n",
    "for category in val_df['Category']:\n",
    "    labels.append(category)\n",
    "    \n",
    "# Store the labeled test data in the following list.\n",
    "labeled_val_data = []\n",
    "    \n",
    "# Going through the two lists in parallel to create the labeled data set.\n",
    "for (l, s) in zip(labels, corpus):\n",
    "\n",
    "    # Convert the original sentence into a vector.\n",
    "    vector = dictionary.doc2bow(s)\n",
    "    \n",
    "    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    \n",
    "    # Add the labeled sentence to the labeled data set.\n",
    "    labeled_val_data.append((sent_as_dict, l))\n",
    "    \n",
    "#print(corpus)    \n",
    "val_tf_vectors = [dictionary.doc2bow(title) for title in corpus]\n",
    "\n",
    "# Convert documents into dict representation. This is document-label representation\n",
    "val_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in val_tf_vectors]\n",
    "val_labels = labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -4.06044        0.000\n",
      "             2          -1.75992        0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1392: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1394: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1395: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.669\n"
     ]
    }
   ],
   "source": [
    "classifier_naive = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifier_maxent = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction for testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_for_naive = []\n",
    "prediction_for_maxent = []\n",
    "goal_category = test_labels\n",
    "#For each file, classify and print the label.\n",
    "for i in range(len(test_df)):\n",
    "    prediction_for_naive.append(classifier_naive.classify(test_data_as_dict[i]))\n",
    "    prediction_for_maxent.append(classifier_maxent.classify(test_data_as_dict[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for naive bayes algorithm:  0.609677249986874\n",
      "Accuracy for maxent algorithm:  0.6470451459988149\n",
      "Precision for naive bayes algorithm 0.53221047910089\n",
      "Precision for maxent algorithm 0.6553976966644761\n",
      "Recall for naive bayes algorithm 0.6636719238012163\n",
      "Recall for maxent algorithm 0.42818639941411446\n",
      "F1 score for naive bayes algorithm 0.5542114758093354\n",
      "F1 score for maxent algorithm 0.46781032688531593\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy.\n",
    "#print(\"Accuracy for naive bayes algorithm: \", nltk.classify.accuracy(classifier_naive, labeled_test_data))\n",
    "#print(\"Accuracy for maxent algorithm: \", nltk.classify.accuracy(classifier_maxent, labeled_test_data))\n",
    "\n",
    "print (\"Accuracy for naive bayes algorithm: \", accuracy_score(goal_category, prediction_for_naive))\n",
    "print (\"Accuracy for maxent algorithm: \", accuracy_score(goal_category, prediction_for_maxent))\n",
    "\n",
    "#precision\n",
    "print(\"Precision for naive bayes algorithm\",precision_score(goal_category, prediction_for_naive , average='macro'))\n",
    "print(\"Precision for maxent algorithm\",precision_score(goal_category, prediction_for_maxent , average='macro'))\n",
    "\n",
    "#recall\n",
    "print(\"Recall for naive bayes algorithm\",recall_score(goal_category, prediction_for_naive , average='macro'))\n",
    "print(\"Recall for maxent algorithm\",recall_score(goal_category, prediction_for_maxent , average='macro'))\n",
    "\n",
    "\n",
    "#f1 score\n",
    "print(\"F1 score for naive bayes algorithm\",f1_score(goal_category, prediction_for_naive , average='macro'))\n",
    "print(\"F1 score for maxent algorithm\",f1_score(goal_category, prediction_for_maxent , average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction for validating</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_for_naive = []\n",
    "prediction_for_maxent = []\n",
    "goal_category = val_labels\n",
    "#For each file, classify and print the label.\n",
    "for i in range(len(test_df)):\n",
    "    prediction_for_naive.append(classifier_naive.classify(val_data_as_dict[i]))\n",
    "    prediction_for_maxent.append(classifier_maxent.classify(val_data_as_dict[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validating Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the accuracy.\n",
    "#print(\"Accuracy for naive bayes algorithm: \", nltk.classify.accuracy(classifier_naive, labeled_test_data))\n",
    "#print(\"Accuracy for maxent algorithm: \", nltk.classify.accuracy(classifier_maxent, labeled_test_data))\n",
    "\n",
    "print (\"Accuracy for naive bayes algorithm: \", accuracy_score(goal_category, prediction_for_naive))\n",
    "print (\"Accuracy for maxent algorithm: \", accuracy_score(goal_category, prediction_for_maxent))\n",
    "\n",
    "#precision\n",
    "print(\"Precision for naive bayes algorithm\",precision_score(goal_category, prediction_for_naive , average='macro'))\n",
    "print(\"Precision for maxent algorithm\",precision_score(goal_category, prediction_for_maxent , average='macro'))\n",
    "\n",
    "#recall\n",
    "print(\"Recall for naive bayes algorithm\",recall_score(goal_category, prediction_for_naive , average='macro'))\n",
    "print(\"Recall for maxent algorithm\",recall_score(goal_category, prediction_for_maxent , average='macro'))\n",
    "\n",
    "\n",
    "#f1 score\n",
    "print(\"F1 score for naive bayes algorithm\",f1_score(goal_category, prediction_for_naive , average='macro'))\n",
    "print(\"F1 score for maxent algorithm\",f1_score(goal_category, prediction_for_maxent , average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
