{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Before closing, go to Cell > All Output > Clear to keep file size small.\n",
    "\n",
    "Also make sure this jupyter notebook file is opened using the following command:\n",
    "\n",
    "```jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes around 7 minutes\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "all_df = pd.read_csv(\"train.csv\",header = 0)\n",
    "corpus = all_df[\"title\"]\n",
    "\n",
    "start = time. time()\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def corpus2docs(corpus):\n",
    "    # corpus is a object returned by load_corpus that represents a corpus.\n",
    "    docs1 = []\n",
    "\n",
    "    for title in corpus:\n",
    "        doc = nltk.word_tokenize(title)\n",
    "        docs1.append(doc)\n",
    "    docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "    docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "    docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "    docs5 = [[stemmer.stem(w) for w in doc] for doc in docs4]\n",
    "    return docs5\n",
    "\n",
    "docs = corpus2docs(corpus)\n",
    "end = time. time()\n",
    "print(end - start)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes around 2 minutes\n",
    "start = time. time()\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "def docs2vecs(docs, dictionary):\n",
    "    # docs is a list of documents returned by corpus2docs.\n",
    "    # dictionary is a gensim.corpora.Dictionary object.\n",
    "    vecs1 = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    tfidf = gensim.models.TfidfModel(vecs1)\n",
    "    vecs2 = [tfidf[vec] for vec in vecs1]\n",
    "    return vecs2\n",
    "\n",
    "vecs = docs2vecs(docs, dictionary)\n",
    "end = time. time()\n",
    "print(end - start)\n",
    "print(vecs)\n",
    "# print(vecs[2])\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes around 15 mins\n",
    "import k_means\n",
    "\n",
    "start = time. time()\n",
    "num_tokens = len(dictionary.token2id)\n",
    "K = 3\n",
    "clusters = k_means.k_means(vecs, num_tokens, K)\n",
    "\n",
    "end = time. time()\n",
    "print(end - start)\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes around 1.5 hours\n",
    "start = time. time()\n",
    "all_df_copy = all_df.copy() # make a copy to avoid modifying original df\n",
    "\n",
    "for k in range(K):\n",
    "    N = len(clusters[k])\n",
    "    for n in range(N):\n",
    "        all_df_copy.loc[clusters[k][n], str(K) + 'cluster'] = k + 1\n",
    "\n",
    "all_df_copy.to_csv(\"clustered3.csv\", index=False)\n",
    "end = time. time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add:\n",
    "\n",
    "Re-run above code without shuffling (do not shuffle as it affects)\n",
    "\n",
    "Add checking column for 3 categories label\n",
    "\n",
    "Repeat code below and run for 58 categories\n",
    "\n",
    "Check performance\n",
    "\n",
    "Add some performance calculator (?)\n",
    "\n",
    "Complete interpreting the clusters section\n",
    "\n",
    "Consider using translator library for malay words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Clusters \n",
    "\n",
    "Incomplete for both K=3 and K=58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "\n",
    "# Take all the file IDs in cluster1\n",
    "cluster1_fids = [fids[d] for d in cluster1]\n",
    "\n",
    "# Create an empty list, clust1_words =[]\n",
    "clust1_words =[]\n",
    "\n",
    "# Add the words from all files to this list. Use corpus.words and extend method. \n",
    "#print(cluster1_fids)\n",
    "clust1_words = [corpus.words(fid)for fid in cluster1_fids]\n",
    "clust1_words = corpus.words(cluster1_fids)\n",
    "#print(clust1_words[0:10])\n",
    "\n",
    "# Remove the Stopwords from this list\n",
    "import re\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "clust1_r_stopwords = [w for w in clust1_words if w not in stop_list]\n",
    "docs2 = [w.lower() for w in clust1_r_stopwords] \n",
    "docs3 = [w for w in docs2 if re.search('^[a-z]+$', w)] \n",
    "clust1_r_process = [stemmer.stem(w) for w in docs3] \n",
    "\n",
    "\n",
    "\n",
    "#Call freq distribution metod and display top 10 words or 20 words\n",
    "import nltk\n",
    "fdist= nltk.FreqDist(clust1_r_stopwords)\n",
    "print(fdist.most_common(10))\n",
    "\n",
    "fdist= nltk.FreqDist(clust1_r_process)\n",
    "print(fdist.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
